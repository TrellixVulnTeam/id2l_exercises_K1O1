{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have implemented several pieces of a deep learning pipeline and even trained a two-layer neural network, but all the hyperparameters were already set to some values yielding resonable results. In real-life problems, however, much of the work in a deep learning project will be geared towards finding the best hyperparameters for a certain problem. In this notebook we will explore some good practices for network debugging and hyperparameters search, as well as extending our previously binary classification neural network to a multi-class one.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some lengthy setup.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from exercise_code.networks.layer import (\n",
    "    Sigmoid, \n",
    "    Relu, \n",
    "    LeakyRelu, \n",
    "    Tanh,\n",
    ")\n",
    "from exercise_code.data import (\n",
    "    DataLoader,\n",
    "    ImageFolderDataset,\n",
    "    RescaleTransform,\n",
    "    NormalizeTransform,\n",
    "    FlattenTransform,\n",
    "    ComposeTransform,\n",
    ")\n",
    "from exercise_code.networks import (\n",
    "    ClassificationNet,\n",
    "    BCE,\n",
    "    CrossEntropyFromLogits\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Quick recap (and some new things)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, in the previous exercises, we focused on building and understanding all the necessary modules for training a simple model. We followed the Pytorch implementations closely, as this is the framework we will use later and we want you to have a smoother transition to its APIs. \n",
    "\n",
    "In the figure below you can see the main components in Pytorch. Before starting the actual exercise, we begin with a quick recap of **our implementation** of these components. \n",
    "\n",
    "Everything is already implemented for this part, but we **strongly** encourage you to check out the respective source files in order to have a better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*uZrS4KjAuSJQIJPgOiaJUg.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation represents an important part of deep learning projects. The data comes from different sources and in different formats and is prepared differently from application to application. One part, however, is clear: because entire datasets are usually too large for us to handle at once, we train our models on smaller batches of data. \n",
    "\n",
    "The goal of the ```Dataset``` class is to encapsulate all the 'dirty' data processing: loading and cleaning the data, storing features (or names of files where features can be found) and labels, as well as providing the means for accessing individual (transformed) items of the data using the ```__getitem__()``` function and an index. You already implemented an ```ImageFolderDataset``` (in ```exercise_code/data/image_folder_dataset.py```) class in Exercise 3. We we will reuse this class here.\n",
    "\n",
    "For processing the data, you implemented several transforms in Exercise 3 (```RescaleTransform```, ```NormalizeTransform```, ```ComposeTransform```). In this exercise we are working with images, which are multidimensional arrays, but we are using simple feedforward neural network which takes a one dimensional array as an input, so it is necessary to reshape the images before feeding them into the model. We implemented this reshape operation for you in the ```FlattenTransform``` class, also found in ```exercise_code/data/image_folder_dataset.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = \"https://cdn3.vision.in.tum.de/~dl4cv/cifar10.zip\"\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "cifar_root = os.path.join(i2dl_exercises_path, \"datasets\", \"cifar10\")\n",
    "\n",
    "# Use the Cifar10 mean and standard deviation computed in Exercise 3.\n",
    "cifar_mean = np.array([0.49191375, 0.48235852, 0.44673872])\n",
    "cifar_std  = np.array([0.24706447, 0.24346213, 0.26147554])\n",
    "\n",
    "# Define all the transforms we will apply on the images when \n",
    "# retrieving them.\n",
    "rescale_transform = RescaleTransform()\n",
    "normalize_transform = NormalizeTransform(\n",
    "    mean=cifar_mean,\n",
    "    std=cifar_std\n",
    ")\n",
    "flatten_transform = FlattenTransform()\n",
    "compose_transform = ComposeTransform([rescale_transform, \n",
    "                                      normalize_transform,\n",
    "                                      flatten_transform])\n",
    "\n",
    "# Create a train, validation and test dataset.\n",
    "datasets = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataset = ImageFolderDataset(\n",
    "        mode=mode,\n",
    "        root=cifar_root, \n",
    "        download_url=download_url,\n",
    "        transform=compose_transform,\n",
    "        split={'train': 0.6, 'val': 0.2, 'test': 0.2}\n",
    "    )\n",
    "    datasets[mode] = crt_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, based on this ```Dataset``` object, we can construct a ```Dataloader``` object which samples a random mini-batch of data at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader for each split.\n",
    "dataloaders = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataloader = DataLoader(\n",
    "        dataset=datasets[mode],\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    dataloaders[mode] = crt_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the ```Dataloader``` has the ```__iter__()``` method, we can simply iterate through the batches it produces, like this:\n",
    "\n",
    "```python\n",
    "for batch in dataloader['train']:\n",
    "    do_something(batch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that the data is prepared, we can discuss the model in which we are feeding the data, which in our case will be a neural network. \n",
    "\n",
    "In Exercise 5, you implemented a simple 2-layer neural network that had a hidden size as a parameter:\n",
    "\n",
    "$$ \n",
    "{\\hat{y}} = \\sigma(\\sigma({x W_1} + {b_1}) {W_2} + {b_2}) \n",
    "$$\n",
    "\n",
    "where $ \\sigma({x}) $ was the sigmoid function, $ {x} $ was the input, $ {W_1}, {W_2} $ the weight matrices and $ {b_1}, {b_2}$ the biases for the two layers.\n",
    "\n",
    "This is how we used this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = datasets['train'][0]['image'].shape[0]\n",
    "model = ClassificationNet(input_size=input_size, \n",
    "                          hidden_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we updated the ```ClassificationNet``` from the previous exercise, so that now you can customize more: the number of outputs, the choice of activation function, the hidden size etc. We encourage you to check out the implementation in ```exercise_code/networks/classification_net.py``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = 2\n",
    "reg = 0.1\n",
    "\n",
    "model = ClassificationNet(activation=Sigmoid(), \n",
    "                          num_layer=num_layer, \n",
    "                          reg=reg,\n",
    "                          num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the forward and backward passes through the model were simply:\n",
    "\n",
    "```python\n",
    "\n",
    "# X is a batch of training features \n",
    "# X.shape = (batch_size, features_size)\n",
    "y_out = model.forward(X)\n",
    "\n",
    "# dout is the gradient of the loss function w.r.t the output of the network.\n",
    "# dout.shape = (batch_size, )\n",
    "model.backward(dout)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the learning rate or the number of iterations we want to train for, the number of hidden layers and the number of units in each hidden layer are also hyperparameters. In this notebook you will play with networks of different sizes and will see the impact that the network capacity has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure how well a network is performing, we implemented several ```Loss``` classes (```L1```, ```MSE```, ```BCE```, each preferred for a certain type of problems) in ```exercise_code/networks/loss.py```.\n",
    "\n",
    "Each implemented a ```forward()``` method, which outputs a number that we use as a proxy for our network performance. \n",
    "\n",
    "Also, because our goal was to change the weights of the network such that this loss measure decreases, we were also interested in the gradients of the loss w.r.t the outputs of the network, $ \\nabla_{\\hat{y}} L({\\hat{y}}, {y}) $. This was implemented in ```backward()```. \n",
    "\n",
    "In previous exercises, we only worked with binary classification and used binary cross entropy (```BCE```) as a loss function.\n",
    "\n",
    "$$ BCE(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^N \\Big [-y_i \\log(\\hat{y_i}) - (1-y_i) \\log(1 - \\hat{y_i}) \\Big] $$ \n",
    "\n",
    "where\n",
    "- $ N $ was the number of samples we were considering\n",
    "- $\\hat{y}_i$ was the network's prediction for sample $i$. Note that this was a valid probability $\\in [0, 1]$, because we applied a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation on the last layer. \n",
    "- $ y_i $ was the ground truth label (0 or 1, depending on the class)\n",
    "\n",
    "Because we have 10 classes in the CIFAR10 dataset, we need a generalization of the binary cross entropy for multiple classes. This is simply called the cross entropy loss and has the following definition:\n",
    "\n",
    "$$ CE(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^{C} \\Big[ -y_{ik} \\log(\\hat{y}_{ik}) \\Big] $$\n",
    "\n",
    "where:\n",
    "- $ N $ is again the number of samples\n",
    "- $ C $ is the number of classes\n",
    "- $ \\hat{y}_{ik} $ is the probability that the model assigns for the $k$th class when the $i$th sample is the input. **Because we don't apply any activation function on the last layer of our network, its outputs for each sample will not be a valid probability distribution over the classes. We call these raw outputs of the network '[logits](https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean/31045)' and we will apply a [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation in order to obtain a valid probability distribution.** \n",
    "- $y_{ik} = 1 $ iff the true label of the $i$th sample is $k$ and 0 otherwise. This is called a [one-hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/).\n",
    "\n",
    "You can check for yourself that when the number of classes $ C $ is 2, then the binary cross entropy is actually equivalent to the cross entropy.\n",
    "\n",
    "We have already implemented the ```CrossEntropyFromLogits``` for you. You can check it out in ```exercise_code/networks/loss.py ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyFromLogits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the previous losses we have seen, we can simply get the results of the forward and backward passes as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# y_out is the output of the neural network\n",
    "# y_truth is the actual label from the dataset\n",
    "loss.forward(y_out, y_truth)\n",
    "loss.backward(y_out, y_truth)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, knowing the gradient of the loss w.r.t the ouputs of the network, as well as the local gradient for each layer of the network, we can use the chain rule to compute all gradients. \n",
    "\n",
    "We implemented several optimizer classes (```SGD```, ```Adam```, ```sgd_momentum```, which you can check out in ```exercise_code/networks/optimizer.py```) that implement different first-order parameter update rules. The ```step()``` method iterates through all the parameters of a model and updates them using the gradient information.\n",
    "\n",
    "What the optimizer is doing, in pseudocode, is the following:\n",
    "\n",
    "```python\n",
    "for param in model:\n",
    "    # Use the gradient to update the weights.\n",
    "    update(param)\n",
    "    \n",
    "    # Reset the gradient after each update.\n",
    "    param.gradient = 0\n",
    "```\n",
    "\n",
    "```SGD``` had the simplest update rule:\n",
    "```python\n",
    "def update(param):\n",
    "    param = param - learning_rate * param.gradient\n",
    "```\n",
    "\n",
    "For the more complicated update rules, see ```exercise_code/networks/optimizer.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Solver``` is where all the above elements come together: given a train and a validation dataloader, a model, a loss and an optimizer, it uses the training data to optimize a model in order to get better predictions. We simply call ```train()``` and it does its 'magic' for us!\n",
    "```python\n",
    "solver = Solver(model, \n",
    "                dataloaders['train'], \n",
    "                dataloaders['val'], \n",
    "                learning_rate=0.001, \n",
    "                loss_func=MSE(), \n",
    "                optimizer=SGD)\n",
    "\n",
    "solver.train(epochs=epochs)\n",
    "```\n",
    "\n",
    "In order to see, that there is no actual 'magic' check out its implementation in ```exercise_code/solver.py``` ;)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. An overview of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "<img src=https://images.deepai.org/glossary-terms/05c646fe1676490aa0b8cab0732a02b2/hyperparams.png alt=hyperparameter width=700>\n",
    "\n",
    "A **hyperparameter** is a parameter that is set before the learning process begins. Recall that the parameters of weight matrix and bias vector are learned during the learning process.\n",
    "\n",
    "The hyperparameters are essential, for they control and affect the whole training and have a great impact on the performance of the model. \n",
    "\n",
    "Some examples of hyperparameters we have covered in lectures:\n",
    "* Network architecture\n",
    "    * Choice of activation function\n",
    "    * Number of layers\n",
    "    * ...\n",
    "* Learning rate\n",
    "* Number of epochs\n",
    "* Batch size\n",
    "* Regularization strength\n",
    "* Momentum\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start debugging your own network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already suggested in the lectures, you may always want to start from small and simple architectures, to make sure you are going the right way. \n",
    "\n",
    "First you may need to overfit a single training sample, then a few batches of training samples, then go deeper with larger neural networks and the whole training data.\n",
    "\n",
    "Here we always provide a default neural network (i.e. ClassificationNet) with arbitrary number of layers, which is a generalization from a fixed 2-layer neural network in exercise 5. You are welcome to implement your own network, in that case just implement **MyOwnNetwork** in ```exercise_code/networks/classification_net.py```. You can also copy things from ClassficationNet and make a little adjustment to your own network. For either way, just pick a network and comment out the other one, then run the cells below for debugging.\n",
    "\n",
    "__Note__: \n",
    "- Please, make sure you don't modify the ClassificationNet itself so that you can always have a working network to fall back on\n",
    "- In order to pass this submissions, you can **first stick to the default ClassificationNet implementation without changing any code at all**. The goal of this submission is to find reasonable hyperparameters and the parameter options of the ClassificationNet are broad enough.\n",
    "- Once you have surpassed the submission goal, you can try to implement additional activation functions in the accompanying notebook, try different weight initializations or other adjustments by writing your own network architecture in the MyOwnNetwork class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's begin with a 2-layer neural network, and overfit one single training sample.\n",
    "\n",
    "After training, let's evaluate the training process by plotting the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 20) train loss: 2.332298; val loss: 2.333434\n",
      "(Epoch 2 / 20) train loss: 2.332298; val loss: 2.321147\n",
      "(Epoch 3 / 20) train loss: 2.207704; val loss: 2.329631\n",
      "(Epoch 4 / 20) train loss: 2.063199; val loss: 2.353444\n",
      "(Epoch 5 / 20) train loss: 1.897424; val loss: 2.384609\n",
      "(Epoch 6 / 20) train loss: 1.729806; val loss: 2.415315\n",
      "(Epoch 7 / 20) train loss: 1.572151; val loss: 2.440568\n",
      "(Epoch 8 / 20) train loss: 1.425705; val loss: 2.459068\n",
      "(Epoch 9 / 20) train loss: 1.288198; val loss: 2.472408\n",
      "(Epoch 10 / 20) train loss: 1.157970; val loss: 2.483585\n",
      "(Epoch 11 / 20) train loss: 1.034278; val loss: 2.495490\n",
      "(Epoch 12 / 20) train loss: 0.918172; val loss: 2.510058\n",
      "(Epoch 13 / 20) train loss: 0.811920; val loss: 2.528006\n",
      "(Epoch 14 / 20) train loss: 0.717184; val loss: 2.549076\n",
      "(Epoch 15 / 20) train loss: 0.633949; val loss: 2.572525\n",
      "(Epoch 16 / 20) train loss: 0.561114; val loss: 2.597548\n",
      "(Epoch 17 / 20) train loss: 0.497982; val loss: 2.623575\n",
      "(Epoch 18 / 20) train loss: 0.444226; val loss: 2.650372\n",
      "(Epoch 19 / 20) train loss: 0.398835; val loss: 2.677988\n",
      "(Epoch 20 / 20) train loss: 0.360317; val loss: 2.706603\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.optimizer import SGD, Adam\n",
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 20\n",
    "reg = 0.1\n",
    "batch_size = 4\n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "# Make a new data loader with a single training image\n",
    "overfit_dataset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=1\n",
    ")\n",
    "dataloaders['train_overfit_single_image'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# Decrease validation data for only debugging\n",
    "debugging_validation_dataset = ImageFolderDataset(\n",
    "    mode='val',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=100\n",
    ")\n",
    "dataloaders['val_500files'] = DataLoader(\n",
    "    dataset=debugging_validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "solver = Solver(model, dataloaders['train_overfit_single_image'], dataloaders['val_500files'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuray: 1.00000\n",
      "Validation accuray: 0.07000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['train_overfit_single_image'])))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val_500files'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time let's try to overfit to a small set of training batch samples. Please observe the difference from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 100) train loss: 2.302409; val loss: 2.302641\n",
      "(Epoch 2 / 100) train loss: 2.308598; val loss: 2.324614\n",
      "(Epoch 3 / 100) train loss: 2.024670; val loss: 2.489914\n",
      "(Epoch 4 / 100) train loss: 1.291812; val loss: 2.836058\n",
      "(Epoch 5 / 100) train loss: 0.767633; val loss: 3.404580\n",
      "(Epoch 6 / 100) train loss: 0.481261; val loss: 3.928927\n",
      "(Epoch 7 / 100) train loss: 0.178865; val loss: 4.410396\n",
      "(Epoch 8 / 100) train loss: 0.076893; val loss: 4.831634\n",
      "(Epoch 9 / 100) train loss: 0.024122; val loss: 5.208886\n",
      "(Epoch 10 / 100) train loss: 0.011009; val loss: 5.524671\n",
      "(Epoch 11 / 100) train loss: 0.004608; val loss: 5.777980\n",
      "(Epoch 12 / 100) train loss: 0.002794; val loss: 5.977752\n",
      "(Epoch 13 / 100) train loss: 0.001333; val loss: 6.135122\n",
      "(Epoch 14 / 100) train loss: 0.000726; val loss: 6.257348\n",
      "(Epoch 15 / 100) train loss: 0.000530; val loss: 6.351392\n",
      "(Epoch 16 / 100) train loss: 0.000406; val loss: 6.423840\n",
      "(Epoch 17 / 100) train loss: 0.000320; val loss: 6.479280\n",
      "(Epoch 18 / 100) train loss: 0.000235; val loss: 6.521609\n",
      "(Epoch 19 / 100) train loss: 0.000206; val loss: 6.553908\n",
      "(Epoch 20 / 100) train loss: 0.000210; val loss: 6.578401\n",
      "(Epoch 21 / 100) train loss: 0.000173; val loss: 6.597152\n",
      "(Epoch 22 / 100) train loss: 0.000168; val loss: 6.611504\n",
      "(Epoch 23 / 100) train loss: 0.000145; val loss: 6.622534\n",
      "(Epoch 24 / 100) train loss: 0.000143; val loss: 6.630967\n",
      "(Epoch 25 / 100) train loss: 0.000140; val loss: 6.637604\n",
      "(Epoch 26 / 100) train loss: 0.000136; val loss: 6.642835\n",
      "(Epoch 27 / 100) train loss: 0.000170; val loss: 6.646924\n",
      "(Epoch 28 / 100) train loss: 0.000162; val loss: 6.650259\n",
      "(Epoch 29 / 100) train loss: 0.000121; val loss: 6.653140\n",
      "(Epoch 30 / 100) train loss: 0.000128; val loss: 6.655461\n",
      "(Epoch 31 / 100) train loss: 0.000123; val loss: 6.657494\n",
      "(Epoch 32 / 100) train loss: 0.000146; val loss: 6.659241\n",
      "(Epoch 33 / 100) train loss: 0.000136; val loss: 6.660812\n",
      "(Epoch 34 / 100) train loss: 0.000109; val loss: 6.662329\n",
      "(Epoch 35 / 100) train loss: 0.000113; val loss: 6.663680\n",
      "(Epoch 36 / 100) train loss: 0.000109; val loss: 6.664982\n",
      "(Epoch 37 / 100) train loss: 0.000108; val loss: 6.666196\n",
      "(Epoch 38 / 100) train loss: 0.000116; val loss: 6.667428\n",
      "(Epoch 39 / 100) train loss: 0.000115; val loss: 6.668546\n",
      "(Epoch 40 / 100) train loss: 0.000104; val loss: 6.669718\n",
      "(Epoch 41 / 100) train loss: 0.000106; val loss: 6.670851\n",
      "(Epoch 42 / 100) train loss: 0.000102; val loss: 6.671909\n",
      "(Epoch 43 / 100) train loss: 0.000107; val loss: 6.672983\n",
      "(Epoch 44 / 100) train loss: 0.000098; val loss: 6.674048\n",
      "(Epoch 45 / 100) train loss: 0.000106; val loss: 6.675012\n",
      "(Epoch 46 / 100) train loss: 0.000096; val loss: 6.675968\n",
      "(Epoch 47 / 100) train loss: 0.000105; val loss: 6.676898\n",
      "(Epoch 48 / 100) train loss: 0.000105; val loss: 6.677764\n",
      "(Epoch 49 / 100) train loss: 0.000102; val loss: 6.678670\n",
      "(Epoch 50 / 100) train loss: 0.000109; val loss: 6.679659\n",
      "(Epoch 51 / 100) train loss: 0.000095; val loss: 6.680611\n",
      "(Epoch 52 / 100) train loss: 0.000111; val loss: 6.681608\n",
      "(Epoch 53 / 100) train loss: 0.000095; val loss: 6.682583\n",
      "(Epoch 54 / 100) train loss: 0.000089; val loss: 6.683538\n",
      "(Epoch 55 / 100) train loss: 0.000105; val loss: 6.684461\n",
      "(Epoch 56 / 100) train loss: 0.000092; val loss: 6.685391\n",
      "(Epoch 57 / 100) train loss: 0.000098; val loss: 6.686303\n",
      "(Epoch 58 / 100) train loss: 0.000090; val loss: 6.687198\n",
      "(Epoch 59 / 100) train loss: 0.000090; val loss: 6.688042\n",
      "(Epoch 60 / 100) train loss: 0.000094; val loss: 6.688888\n",
      "(Epoch 61 / 100) train loss: 0.000093; val loss: 6.689700\n",
      "(Epoch 62 / 100) train loss: 0.000081; val loss: 6.690555\n",
      "(Epoch 63 / 100) train loss: 0.000089; val loss: 6.691354\n",
      "(Epoch 64 / 100) train loss: 0.000089; val loss: 6.692249\n",
      "(Epoch 65 / 100) train loss: 0.000088; val loss: 6.693230\n",
      "(Epoch 66 / 100) train loss: 0.000083; val loss: 6.694254\n",
      "(Epoch 67 / 100) train loss: 0.000088; val loss: 6.695265\n",
      "(Epoch 68 / 100) train loss: 0.000096; val loss: 6.696236\n",
      "(Epoch 69 / 100) train loss: 0.000096; val loss: 6.697262\n",
      "(Epoch 70 / 100) train loss: 0.000082; val loss: 6.698334\n",
      "(Epoch 71 / 100) train loss: 0.000077; val loss: 6.699370\n",
      "(Epoch 72 / 100) train loss: 0.000079; val loss: 6.700289\n",
      "(Epoch 73 / 100) train loss: 0.000078; val loss: 6.701203\n",
      "(Epoch 74 / 100) train loss: 0.000077; val loss: 6.702087\n",
      "(Epoch 75 / 100) train loss: 0.000086; val loss: 6.703028\n",
      "(Epoch 76 / 100) train loss: 0.000082; val loss: 6.704011\n",
      "(Epoch 77 / 100) train loss: 0.000096; val loss: 6.704876\n",
      "(Epoch 78 / 100) train loss: 0.000075; val loss: 6.705793\n",
      "(Epoch 79 / 100) train loss: 0.000082; val loss: 6.706743\n",
      "(Epoch 80 / 100) train loss: 0.000071; val loss: 6.707605\n",
      "(Epoch 81 / 100) train loss: 0.000074; val loss: 6.708501\n",
      "(Epoch 82 / 100) train loss: 0.000079; val loss: 6.709326\n",
      "(Epoch 83 / 100) train loss: 0.000081; val loss: 6.710173\n",
      "(Epoch 84 / 100) train loss: 0.000080; val loss: 6.710944\n",
      "(Epoch 85 / 100) train loss: 0.000071; val loss: 6.711738\n",
      "(Epoch 86 / 100) train loss: 0.000081; val loss: 6.712537\n",
      "(Epoch 87 / 100) train loss: 0.000077; val loss: 6.713437\n",
      "(Epoch 88 / 100) train loss: 0.000074; val loss: 6.714300\n",
      "(Epoch 89 / 100) train loss: 0.000076; val loss: 6.715130\n",
      "(Epoch 90 / 100) train loss: 0.000079; val loss: 6.715888\n",
      "(Epoch 91 / 100) train loss: 0.000085; val loss: 6.716689\n",
      "(Epoch 92 / 100) train loss: 0.000075; val loss: 6.717576\n",
      "(Epoch 93 / 100) train loss: 0.000075; val loss: 6.718447\n",
      "(Epoch 94 / 100) train loss: 0.000073; val loss: 6.719324\n",
      "(Epoch 95 / 100) train loss: 0.000068; val loss: 6.720132\n",
      "(Epoch 96 / 100) train loss: 0.000073; val loss: 6.720944\n",
      "(Epoch 97 / 100) train loss: 0.000065; val loss: 6.721718\n",
      "(Epoch 98 / 100) train loss: 0.000074; val loss: 6.722508\n",
      "(Epoch 99 / 100) train loss: 0.000072; val loss: 6.723335\n",
      "(Epoch 100 / 100) train loss: 0.000070; val loss: 6.724140\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 100\n",
    "reg = 0.1\n",
    "num_samples = 10\n",
    "\n",
    "# model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "model = MyOwnNetwork()\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "# Make a new data loader with a our num_samples training image\n",
    "overfit_dataset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=num_samples\n",
    ")\n",
    "dataloaders['train_overfit_10samples'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "solver = Solver(model, dataloaders['train_overfit_10samples'], dataloaders['val_500files'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-d2afa49d2f8f>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-d2afa49d2f8f>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    story, '-', label='val')\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_hi\n",
    "         story, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['train_overfit_10samples'])))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val_500files'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're overfitting the training data, that means the network's implementation is correct. However, as you have more samples to overfit, your accuracy will be way lower. You can increase the number of epochs above to achieve better results.\n",
    "\n",
    "Now let's try to feed all the training and validation data into the network, but this time we set the same hyperparameters for 2-layer and 5-layer networks, and compare the different behaviors.\n",
    "\n",
    "__Note__: This may take about 1 min for each epoch as the training set is quite large. For convenience, we only train on 1000 images for now but use the full validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 5\n",
    "reg = 0.01\n",
    "\n",
    "# Make a new data loader with 1000 training samples\n",
    "num_samples = 1000\n",
    "overfit_dataset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=num_samples\n",
    ")\n",
    "dataloaders['train_small'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Change here if you want to use the full training set\n",
    "use_full_training_set = False\n",
    "if not use_full_training_set:\n",
    "    train_loader = dataloaders['train_small']\n",
    "else:\n",
    "    train_loader = dataloaders['train']\n",
    "    \n",
    "\n",
    "# model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "model = MyOwnNetwork(num_layer=num_layer, reg=reg)\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "solver = Solver(model, train_loader, dataloaders['val'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(train_loader)))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 5\n",
    "epochs = 5\n",
    "reg = 0.01\n",
    "\n",
    "# model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "model = MyOwnNetwork(num_layer=num_layer, reg=reg)\n",
    "\n",
    "# Change here if you want to use the full training set\n",
    "use_full_training_set = False\n",
    "if not use_full_training_set:\n",
    "    train_loader = dataloaders['train_small']\n",
    "else:\n",
    "    train_loader = dataloaders['train']\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "solver = Solver(model, train_loader, dataloaders['val'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(train_loader)))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 50\n",
    "reg = 0.001\n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork(num_layer=num_layer, reg=reg)\n",
    "\n",
    "# Change here if you want to use the full training set\n",
    "use_full_training_set = False\n",
    "if not use_full_training_set:\n",
    "    train_loader = dataloaders['train_small']\n",
    "else:\n",
    "    train_loader = dataloaders['train']\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "solver = Solver(model, train_loader, dataloaders['val'], \n",
    "                learning_rate=1e-4, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)\n",
    "\n",
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(train_loader)))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, the same hyperparameter set can decrease the loss for a 2-layer network, but for 5-layer network, it hardly works.\n",
    "\n",
    "The steps above are already mentioned in the lectures as debugging steps before training a neural network. \n",
    "\n",
    "If you implement your own network, make sure you do the steps above before tuning the hyperparameters as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficulty in tuning hyperparameters\n",
    "As can be seen through the results of training a larger network, training with whole data doesn't fit the training data as well as training with small number of training data. Besides, the architecture of neural network makes a difference, too. Small decisions on hyperparameters count. \n",
    "\n",
    "Usually, but not always, hyperparameters cannot be learned using well known gradient based methods (such as gradient descent), which are commonly employed to learn parameters. Besides, some hyperparameters can affect the structure of the model and the loss function.\n",
    "\n",
    "As mentioned before, hyperparameters need to be set before training. Tuning hyperparameters is hard, because you always have to try different combinations of the hyperparameters, train the network, do the validation and pick the best one. Besides, it is not guaranteed that you'll find the best combination.\n",
    "\n",
    "Next you will do hands on learning with hyperparameter tuning methods that are covered in lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "![alt text](https://blog.floydhub.com/content/images/2018/08/Screen-Shot-2018-08-22-at-17.59.25.png \"\")\n",
    "\n",
    "One of the main challenges in deep learning is finding the set of hyperparameters that performs best.\n",
    "\n",
    "So far, we have followed a manual approach by guessing hyperparameters, running the model, observing the result and maybe tweaking the hyperparameters based on this result. As you have probably noticed, this manual hyperparameter tuning is unstructured, inefficient and can become very tedious.\n",
    "\n",
    "\n",
    "A more systematic (and actually very simple) approach for hyperparameter tuning that you've already learned in the lecture  is implementing a **Grid Search**. \n",
    "\n",
    "\n",
    "\n",
    "### Grid Search\n",
    "Grid search is a simple and naive, yet effective method to automate the hyperparameter tuning:\n",
    "\n",
    "* First, you define the set of parameters you want to tune, e.g. $\\{learning\\_rate, regularization\\_strength\\}$.\n",
    "\n",
    "* For each hyperparameter, you then define a set of possible values, e.g. $learning\\_rate = \\{0.0001, 0.001, 0.01, 0.1\\}$.\n",
    "\n",
    "* Then, you train a model for every possible combination of these hyperparameter values and afterwards select the combination that works best (e.g. in terms of accuracy on your validation set).\n",
    "\n",
    "Check out our grid search implementation in `exercise_code/hyperparameter_tuning.py`. We show a simple for loop implementation and a more sophisticated one for multiple inputs.\n",
    "\n",
    "*Note: to keep things simple for the beginning, it'll be enough to just focus on the hyperparameters `learning_rate` and `regularization_strength`\n",
    " here, as in the example above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "# Specify the used network\n",
    "model_class = ClassificationNet\n",
    "\n",
    "from exercise_code import hyperparameter_tuning\n",
    "best_model, results = hyperparameter_tuning.grid_search(\n",
    "    dataloaders['train_small'], dataloaders['val_500files'],\n",
    "    grid_search_spaces = {\n",
    "        \"learning_rate\": [1e-2, 1e-3, 1e-4], \n",
    "        \"reg\": [1e-4]\n",
    "    },\n",
    "    model_class=model_class,\n",
    "    epochs=10, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of your grid search, you might already have found some hyperparameter combinations that work better than others. A common practice is to now repeat the grid search on a more narrow domain centered around the parameters that worked best. \n",
    "\n",
    "**Conclusion Grid Search**\n",
    "\n",
    "With grid search we now have automated the hyperparameter tuning to a certain degree. Another advantage is, that since the training of all models are independent of each other, you can parallelize the grid search, i.e.,  try out different hyperparameter configurations in parallel on different machines.\n",
    "\n",
    "However, as you have probably noticed, there is one big problem with this approach: the number of possible combinations to try out grows exponentially with the number of hyperparameters (\"curse of dimensionality\"). As we add more hyperparameters to the grid search, the search space will explode in time complexity, making this strategy unfeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially when your search space contains more than 3 or 4 dimensions, it is often better to use another, similar hyperparameter tuning method that you've already learned about: random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "Random search is very similar to grid search, with the only difference, that instead of providing specific values for every hyperparameter, you only define a range for each hyperparameter - then, the values are sampled randomly from the provided ranges.\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/cIDuR.png \"\")\n",
    "\n",
    "The figure above illustrates the difference in the hyperparameter space exploration between grid search and random search: assume you have 2 hyperparameters with each 3 values. Running a grid search results in training $3^2=9$ different models - but in the end, you've just tired out 3 values for each parameter. For random search on the other hand, after training 9 models you'll have tried out 9 different values for each hyperparameter, which often leads much faster to good results.\n",
    "\n",
    "To get a deeper understanding of random search and why it is more efficient than grid search, you should definitely check out this paper: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now please implement the `nn_random_search` function in `exercise_code/networks/hyperparameter_tuning.py`. \n",
    "\n",
    "*Hint: regarding the sample space of each parameter, think about the scale for which it makes most sense to sample in. For example the learning rate is usually sampled on a logarithmic scale!*\n",
    "\n",
    "*For simplicity and speed, just use the `train_batches`-dataloader!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.hyperparameter_tuning import random_search\n",
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "# Specify the used network\n",
    "model_class = ClassificationNet\n",
    "\n",
    "best_model, results = random_search(\n",
    "    dataloaders['train_small'], dataloaders['val_500files'],\n",
    "    random_search_spaces = {\n",
    "        \"learning_rate\": ([1e-2, 1e-6], 'log'),\n",
    "        \"reg\": ([1e-3, 1e-7], \"log\"),\n",
    "        \"loss_func\": ([CrossEntropyFromLogits()], \"item\")\n",
    "    },\n",
    "    model_class=model_class,\n",
    "    num_search = 1, epochs=20, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation works, then it's time to run it with the whole dataset, and let it search for a few hours for a nice configuration. \n",
    "\n",
    "However, to save some time, let's first implement an **early-stopping** mechanism, that you also already know from the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you've already seen a lot of training curves:\n",
    "\n",
    "<img src=http://fouryears.eu/wp-content/uploads/2017/12/early_stopping.png></img>\n",
    "\n",
    "Usually, at some point the validation loss goes up again, which is a sign that we're overfitting to our training data. Since it actually doesn't make any sense to train further at this point, it's common practice to apply \"early stopping\", i.e., cancel the training process when the validation loss doesn't improve anymore. The nice thing about this concept is, that not only it improves generalization through the prevention of overfitting, but also it saves us a lot of time - one of our most valuable resources in deep learning.\n",
    "\n",
    "Since there are natural fluctuations in the validation loss, you usually don't cancel the training process right at the first epoch when the validation-loss increases, but instead, you wait for some epochs (specified by the `patience`-parameter) and if the loss still doesn't improve, we stop.\n",
    "\n",
    "Now, take a look at the implement of early stopping mechanism in the `solver.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's find the perfect model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now set everything up to start training your model and finding a nice set of hyper parameters using a combination of grid or random search!\n",
    "\n",
    "Since we'll now be training with a much larger number of samples, you should be aware that this process will definitely take some time! So be prepared to let your machine run for a while. \n",
    "\n",
    "At the beginning, it's a good approach to first do a coarse random search across a wide range of values to find promising sub-ranges of your parameter space. Afterwards, you can zoom in to these ranges and do another random search (or grid search) to finetune the configuration.\n",
    "\n",
    "You don't have to use the whole dataset at the beginning, instead you can also use a medium large subset of the samples. Also, you don't need to train for a large number of epochs - as mentioned above: we first want to get an overview about our hyper parameters.\n",
    "\n",
    "**Task: put in some reasonable ranges for the hyperparameters and evaluate them using random search!**\n",
    "\n",
    "Finally, once you've found some good hyper parameters, train on the whole dataset for a large number of epochs and make sure you use the full training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.hyperparameter_tuning import random_search\n",
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "# Specify the used network\n",
    "model_class = MyOwnNetwork\n",
    "\n",
    "best_model, results = random_search(\n",
    "    dataloaders['train'], dataloaders['val'],\n",
    "    random_search_spaces = {\n",
    "        \"learning_rate\": ([1e-4], 'log'),\n",
    "        \"reg\": ([2e-3], \"log\"),\n",
    "        \"loss_func\": ([CrossEntropyFromLogits()], \"item\")\n",
    "    },\n",
    "    model_class=model_class,\n",
    "    num_search = 1, epochs=150, patience=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to edit the ranges above and adjust them to explore regions that performed well!\n",
    "\n",
    "Also, feel free to experiment around! Also the network architecture, optimizer options and activations functions, etc. are hyperparameters that you can change!\n",
    "\n",
    "Try to get your accuracy as high as possible! That's all what counts for this submission!\n",
    "\n",
    "You'll pass if you reach at least **48%** accuracy on our test set - but there will also be a leaderboard of all students of this course. Will you make it to the top?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['train'])\n",
    "print(\"Train Accuracy: {}%\".format(acc*100))\n",
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['val'])\n",
    "print(\"Validation Accuracy: {}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your model\n",
    "When you have finished your hyperparameter tuning and are sure you have your final model that performs well on the validation set (**you should at least get 48% accuracy on the validation set!**), it's time to run your  model on the test set.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Important</h3>\n",
    "    <p>As you have learned in the lecture, you must only use the test set one single time! So only run the next cell if you are really sure your model works well enough and that you want to submit. Your test set is different from the test set on our server, so results may vary. Nevertheless, you will have a reasonable close approximation about your performance if you only do a final evaluation on the test set.</p>\n",
    "    <p>If you are an external student that can't use our submission webpage: this test performance is your final result and if you surpassed the threshold, you have completed this exercise :). Now, train again to aim for a better number!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment this part out to see your model's performance on the test set.\n",
    "'''\n",
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['test'])\n",
    "print(\"Test Accuracy: {}%\".format(acc*100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The \"real\" test set is actually the dataset we're using for testing your model, which is **different** from the test-set you're using here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "save_pickle({\"cifar_fcn\": best_model}, \"cifar_fcn.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Congratulations! You've just built your first image classifier! To complete the exercise, submit your final model to our submission portal - you probably know the procedure by now.\n",
    "\n",
    "1. Go on [our submission page](https://dvl.in.tum.de/teaching/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an id which we need in the next step.\n",
    "2. Navigate to `exercise_code` directory and run the `create_submission.sh` file to create the zip file of your model. This will create a single `zip` file that you need to upload. Otherwise, you can also zip it manually if you don't want to use the bash script.\n",
    "3. Log into [our submission page](https://dvl.in.tum.de/teaching/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted \"dummy_model.p\" file selectable on the top.\n",
    "4. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold.\n",
    "\n",
    "# Submission Goals\n",
    "\n",
    "- Goal: Successfully implement a fully connected NN image classifier, tune hyperparameters.\n",
    "\n",
    "- Passing Criteria: This time, there are no unit tests that check specific components of your code. The only thing that's required to pass the submission, is your model to reach at least **48% accuracy** on __our__ test dataset. The submission system will show you a number between 0 and 100 which corresponds to your accuracy.\n",
    "\n",
    "- Submission start: __May 28, 2020 12.00__\n",
    "- Submission deadline : __June 03, 2020 23.59__ \n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
