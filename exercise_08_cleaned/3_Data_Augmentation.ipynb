{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "  \n",
    "In this notebook we will show different data augmentation techniques on **CIFAR-10 dataset** using pytorch. As you already learned in class, data augmentation is not only a solution when you don't have enough data to train, but also an effective way to improve the generalization performance of your classifier.\n",
    "\n",
    "Image Augmentation is the process of generating new images for training our deep learning model. These new images are generated using the **existing training images** and hence we donâ€™t have to collect them manually.\n",
    "\n",
    "**HOWEVER**, these techniques do not come entirely for free. You need to consider the 'safety' of your augmentation. The safety of a Data Augmentation method refers to its likelihood of preserving the label post-transformation. For example, rotations and flips are generally safe on ImageNet challenges such as cat versus dog, but not safe for digit recognition tasks such as 6 versus 9. Besides, some augmentation techniques will increase the computation expensiveness and lead to additional training time. So you need to have this in mind before you apply data augmentation to your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to apply Image Augmentation?\n",
    "\n",
    "Image data augmentation can be applied as a pre-processing step before we train the model or can be applied in real time.\n",
    "\n",
    "#### Offline Augmentation\n",
    "\n",
    "Augmentation is applied as a pre-processing step to increase the size of the dataset. This is usually done when we have a small training dataset that we want to expand. Generating augmentation on smaller dataset is helpful but we need to consider the disk space when applying on larger datasets. In this case, the size of the augmented dataset is fixed.\n",
    "\n",
    "\n",
    "#### Online Augmentation\n",
    "As the name suggests, this kind of augmentation is applied in real time. This is usually applied for larger datasets as we do not need to save the augmented images on the disk. In this case, we apply transformations in mini-batches and then feed it to the model. So the size of the augmented dataset that the model actually sees can be infinitly large.\n",
    "\n",
    "In online augmentation, the model will see different images at each epoch. In offline augmentation, the augmented images are part of the training set, so it views the augmented image multiple times depending on the number of epochs. Hence, the model could generalize better with online augmentation as it sees more samples during training with online data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation Techniques\n",
    "\n",
    "Since we will work on the CIFAR-10 dataset, the augmentation techniques mentioned below will work in the context of image classification scenarios.\n",
    "\n",
    "In the following we will show **center crop, random crops, rotation, shifting, flipping, adding noise, resizing, and changing the brightness/contrast/saturation to the images** that work directly on the image object. Also, we will show other common data processing methods such as **toTensor and normalizaion** which works on the tensors that are transformed from images.\n",
    "\n",
    "All these techniques are available in pytorch, and if you design your own network in the future, please consider data augmentation as a part of your network design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an image from CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (4.8, 4.8) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 345.6x345.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We will load an image from the cifar-10 dataset that are already used serval times before on your local machine.\n",
    "\n",
    "original_image = Image.open('../datasets/cifar10/cat/4839.png')\n",
    "plt.imshow(original_image)\n",
    "plt.title('Original Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 ToTensor\n",
    "\n",
    "The very first thing you need to know is that our network takes **tensors** as input, so one of the most important transformation you need to do is to convert a PIL Image or a numpy.ndarray to tensor before you pass them to your model. You can use\n",
    "\n",
    "```torchvision.ToTensor()```\n",
    "\n",
    "which will convert a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8.\n",
    "\n",
    "In the other cases, tensors are returned without scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape before transformation:  (32, 32)\n",
      "\n",
      "The shape after transformation:  torch.Size([3, 32, 32])\n",
      "\n",
      "After the transformation, the image becomes:\n",
      "  tensor([[[0.1804, 0.0235, 0.2353,  ..., 0.4353, 0.3176, 0.1529],\n",
      "         [0.2000, 0.0078, 0.2392,  ..., 0.4353, 0.4000, 0.2039],\n",
      "         [0.2157, 0.0745, 0.2588,  ..., 0.2039, 0.2863, 0.1922],\n",
      "         ...,\n",
      "         [0.2039, 0.2235, 0.4275,  ..., 0.7843, 0.8392, 0.8039],\n",
      "         [0.1529, 0.2471, 0.3804,  ..., 0.8039, 0.8549, 0.8627],\n",
      "         [0.1490, 0.1922, 0.3020,  ..., 0.7412, 0.7059, 0.7765]],\n",
      "\n",
      "        [[0.4235, 0.3686, 0.4235,  ..., 0.9020, 0.7725, 0.5490],\n",
      "         [0.4431, 0.3373, 0.3922,  ..., 0.8706, 0.8510, 0.6471],\n",
      "         [0.4353, 0.3451, 0.3765,  ..., 0.5647, 0.7333, 0.6196],\n",
      "         ...,\n",
      "         [0.2588, 0.2588, 0.4588,  ..., 0.9686, 0.9647, 0.9451],\n",
      "         [0.1961, 0.2824, 0.4235,  ..., 0.9804, 0.9843, 0.9725],\n",
      "         [0.1882, 0.2196, 0.3294,  ..., 0.9686, 0.9608, 0.9608]],\n",
      "\n",
      "        [[0.4824, 0.3255, 0.4745,  ..., 0.9020, 0.7882, 0.5804],\n",
      "         [0.5176, 0.2941, 0.4353,  ..., 0.8824, 0.8824, 0.6863],\n",
      "         [0.5216, 0.3294, 0.4431,  ..., 0.6000, 0.7529, 0.6471],\n",
      "         ...,\n",
      "         [0.4392, 0.4118, 0.5725,  ..., 0.9490, 0.9490, 0.9451],\n",
      "         [0.3804, 0.4392, 0.5569,  ..., 0.9608, 0.9569, 0.9647],\n",
      "         [0.3804, 0.3725, 0.4353,  ..., 0.9451, 0.9412, 0.9490]]])\n"
     ]
    }
   ],
   "source": [
    "transform_0 = transforms.ToTensor()\n",
    "image_transformed_0 = transform_0(original_image)\n",
    "print(\"The shape before transformation: \", original_image.size)\n",
    "print(\"\\nThe shape after transformation: \", image_transformed_0.shape)\n",
    "print(\"\\nAfter the transformation, the image becomes:\\n \", image_transformed_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Normalization\n",
    "\n",
    "This technique is usually applied after the images are tranformed to tensors. Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, \n",
    "\n",
    "```torchvision.transforms.Normalize(mean, std, inplace=False)```\n",
    "\n",
    "will normalize each channel of the input torch.Tensor i.e. *output[channel] = (input[channel] - mean[channel]) / std[channel]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "transform_01 = transforms.Normalize(mean, std, inplace=False)\n",
    "image_transformed_01 = transform_01(image_transformed_0)\n",
    "print(\"After the normalization, you can see the values of the tensor become: \\n\", image_transformed_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Center Crops\n",
    "\n",
    "Now, let's talk about what image augmentation techniques that can apply **directly** on the PIL images.\n",
    "\n",
    "The first one you should know is: center crops.\n",
    "\n",
    "To crop the given PIL Image at the center. You can use \n",
    "\n",
    "```torchvision.transforms.CenterCrop(size)```\n",
    "\n",
    "**Note:** You need to pass in the desired output size into the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_1 = transforms.CenterCrop((20,20))\n",
    "image_transformed_1 = transform_1(original_image)\n",
    "plt.imshow(image_transformed_1)\n",
    "plt.title('Centor Crop')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Crops\n",
    "\n",
    "To crop the given PIL Image at a random location, you can use\n",
    "\n",
    "```torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')```\n",
    "\n",
    "**Note:** You need to pass in the desired output size into the transformation. Other variables are optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_2 = transforms.RandomCrop((20,20))\n",
    "image_transformed_2 = transform_2(original_image)\n",
    "plt.imshow(image_transformed_2)\n",
    "plt.title('Random Crop')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Resized Crops\n",
    "\n",
    "To crop the given PIL Image to random size and aspect ratio, you can use\n",
    "\n",
    "```torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)```\n",
    "\n",
    "**Note:** You need to pass in the desired output size and this crop will finally resized to your given size. This is popularly used to train the Inception networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_3 = transforms.RandomResizedCrop((20,20))\n",
    "image_transformed_3 = transform_3(original_image)\n",
    "plt.imshow(image_transformed_3)\n",
    "plt.title('Random Crop')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Rotation\n",
    "\n",
    "Image rotation is one of the most commonly used augmentation techniques. It can help our model become robust to the changes in the orientation of objects. Even if we rotate the image, the information of the image remains the same. A cat is a cat even if we see it from a different angle.\n",
    "\n",
    "To rotate the given PIL Image by a given range of degrees, you can use\n",
    "\n",
    "```torchvision.transforms.RandomRotation(degrees, resample=False, expand=False, center=None, fill=None)``` \n",
    "\n",
    "**Note:** If you pass in a number instead of a range, the image will be rotated in a angle from (-degree, +degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_4 = transforms.RandomRotation((10,90))\n",
    "image_transformed_4 = transform_4(original_image)\n",
    "plt.imshow(image_transformed_4)\n",
    "plt.title('Random Rotation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Flipping\n",
    "\n",
    "We can flip the image horizontally or vertically.\n",
    "\n",
    "You can use  \n",
    "\n",
    "```torchvision.transforms.RandomHorizontalFlip()``` or ```torchvision.transforms.RandomVerticalFlip()```\n",
    "\n",
    "**Note:**\n",
    "\n",
    "**1.** It takes a float as the input to serve as the probability of the image to be flipped. The default value is 0.5.\n",
    "\n",
    "**2.** On datasets involving text recognition such as MNIST or SVHN, this is not a label-preserving transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal flip\n",
    "transform_5_1 = transforms.RandomHorizontalFlip(p=0.9)\n",
    "image_transformed_5_1 = transform_5_1(original_image)\n",
    "plt.imshow(image_transformed_5_1)\n",
    "plt.title('Random Horizontal Flip')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertical flip\n",
    "transform_5_2 = transforms.RandomVerticalFlip(p=0.9)\n",
    "image_transformed_5_2 = transform_5_2(original_image)\n",
    "plt.imshow(image_transformed_5_2)\n",
    "plt.title('Random Vertical Flip')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Shifting\n",
    "\n",
    "There might be scenarios when the objects in the image are not perfectly central aligned. In these cases, image shift can be used to add shift-invariance to the images.\n",
    "\n",
    "By shifting the images, we can change the position of the object in the image and hence give more variety to the model. This will eventually lead to a more generalized model.\n",
    "\n",
    "To apply random affine transformation to your image, you can use\n",
    "\n",
    "```torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)```\n",
    "\n",
    "**Note:** This transformation takes a range of rotation degrees as input, and you can also pass in other parameters such as translate, which is a tuple of maximum absolute fraction for horizontal and vertical translations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_6 = transforms.RandomAffine(degrees=(10,90), translate=(0.1,0.3))\n",
    "image_transformed_6 = transform_6(original_image)\n",
    "plt.imshow(image_transformed_6)\n",
    "plt.title('Random Affine Transformation')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Adding Gaussian Noise\n",
    "\n",
    "Adding noise to images is an important augmentation step that allows our model to learn how to separate signal from noise in an image. This also makes the model more robust to changes in the input.\n",
    "\n",
    "To realize this, you can use\n",
    "\n",
    "```torchvision.transforms.Lambda(lambd)```\n",
    "\n",
    "to design your own transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_7 = transforms.Compose([transforms.ToTensor(),transforms.Lambda(lambda x : x + 0.1*torch.randn_like(x)),transforms.ToPILImage()])\n",
    "image_transformed_7 = transform_7(original_image)\n",
    "plt.imshow(image_transformed_7)\n",
    "plt.title('Add Noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ColorJitter\n",
    "\n",
    "To randomly change the brightness, contrast and saturation of an image, you can use\n",
    "\n",
    "```torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)```\n",
    "\n",
    "**Note:** All these four arguments take either a float or a tuple as input. And your inputs basically specify a range that the tranformation can apply on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_8 = transforms.ColorJitter(brightness=0.3, contrast=0.6, saturation=0.5)\n",
    "image_transformed_8 = transform_8(original_image)\n",
    "plt.imshow(image_transformed_8)\n",
    "plt.title('Noised Image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Resizing\n",
    "\n",
    "Because our model takes a certain input size, so sometimes we need to resize our images.\n",
    "\n",
    "To resize the input PIL Image to the given size, you can use\n",
    "\n",
    "```torchvision.transforms.Resize(size, interpolation=2)``` \n",
    "\n",
    "**Note:** you need to pass in the desired output size. If size is a sequence like (h, w), output size will be matched to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_9 = transforms.Resize((24,24))\n",
    "image_transformed_9 = transform_9(original_image)\n",
    "plt.imshow(image_transformed_9)\n",
    "plt.title('Resized Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Combine All These Together\n",
    "\n",
    "All these mentioned above are widely used in network design, and you can combine them by using\n",
    "\n",
    "```torchvision.transforms.Compose(transforms)```\n",
    "\n",
    "This can composes several transforms together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((20,20)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.6, saturation=0.5),\n",
    "    transforms.RandomAffine(degrees=(10,90), translate=(0.1,0.3)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std, inplace=False)\n",
    "])\n",
    "image_transformed = transform(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to learn more about data augmentation using Pytorch, you can check it out the documentation below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://pytorch.org/docs/stable/torchvision/transforms.html', width=900, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
